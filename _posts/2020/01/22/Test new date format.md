
## 重点

### 导言

#### 1) 训练神经网络是一个充满"漏洞"的概念

很多博客/教学视频很自豪的宣称“只需要30行代码即可解决数据训练问题”，给大家一种错误的印象即训练一个神经网络非常简单，即插即用

```python
>>> your_data = # 导入你的数据集
>>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)
# 征服世界吧
```

这样的代码和示例让人回想起标准的软件——拥有干净的API接口和抽象——比如[Requests网络库](http://docs.python-requests.org/en/master/)：
```python
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
```

看上去很酷，但其实背后需要很多工作将诸多复杂的细节隐藏在标准库中，只展现几行代码。  
然而神经网络与传统软件天差地别：它们不是标准化(off the shell)，即插即用(plug and play)的技术，
在ImageNet上训练一个分类器很简单，但只要稍稍修改就需要更深刻的理解
（参见原作者的另一篇博客["Yes you should understand backprop"](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b))。  
BackProp+SGD不意味着网络一定收敛；BatchNorm也不代表收敛一定更快；RNNs不代表你可以随意处理任何文本；可以使用RL(Reinforcement Learning)来抽象不见得应该使用RL。 

**如果你坚持使用并不理解的技术，那么你大概率会失败**

#### 2) 训练神经网络会静静的失败

当你代码写错的时候，你会遇到各种各样的异常(Exception)：
 - 参数类型错误
 - 参数树立那个错误
 - import失败
 - 不存在的Key
 - ...

对于神经网络的训练，这些**软件的错误**只是一个开始。

当你搞定了所有的编译错误和语法问题，网络可能还是不能work，这时就很难讲文提出在那里了。问题可能出现的地方太多了，并且是逻辑(logical_问题而不是语法(syntactic)问题，很难通过传统软件中的单元测试解决，比如：
 - 数据增强环节中，你flip了图像，但忘记了对label进行flip——很可能最终网络拟合的很好，它学会了根据图像去flip label...
 - 自回归模型(autoregressive)中你不小心将输出作为输入进行训练
 - 应该对梯度进行clipping却对损失(loss)进行了clipping, 导致训练中直接忽略了样本中的outlier
 - 你导入了预训练的圈中却没有使用原本的均值
 - 超参数配置错误：正则化参数、学习率、模型大小等等

如果你的模型能跑出异常才是撞了大运了，大多数时候它只会默默的掉几个点...
因此，想“多快好省”的训练神经网络是不切实际的，只会带来无尽的痛苦。炼丹的过程痛苦是无法避免的，但遵循以下的原则是可以缓解的：
 - 细致(thorough)
 - 保守(defensive)
 - 多疑/偏执(paranoid)
 - 执着于可视化(一切可能被可视化的环节)

训练神经网络最重要的是**耐心**和**对细节的关注**

### 炼丹大法


当需要训练一个神经网络来解决新的问题时，要从从简到繁的构建并且在每一步先进行坚实的假设，而后利用实验来验证或者深入挖掘到发现问题为止。
在这个过程中要坚决避免同事引入多个“未验证”的难题而导致难以发现的错误。

*如果把炼丹的过程比作炼丹本身，那么你会希望使用很小的学习率，并且每次迭代都在完整的测试集上评估性能*

#### 1. 与数据合二为一

此时不要写任何网络相关的代码，详细的检视你的输入数据。   
**这一步非常重要**  
人脑非常擅长扫描大量的数据，理解他们的分布并且搜寻数据的模式：
 - 重复的图片
 - 错误的图片/标签
 - 不平衡的数据分布
 - 根据你的需求判断，你需要什么样的网络结构？局部的特征就够了还是需要全局的上下文？


 
#### 2. 构建端到端的训练和评估框架 + dummy基线版本









## 机器翻译

几周前我发布一条关于“最常见的神经网络错误”的推文，列出了一些与训练神经网络相关的常见问题。这条推文的参与度比我预期的要高得多(包括网络研讨会:)).显然，很多人个人都遇到过“这里是卷积层的工作原理”和“我们的connet实现了最先进的结果”之间的巨大差距。

所以我想，把我尘土飞扬的博客甩掉，把我的推文扩展到这个话题值得的长形式，这可能会很有趣。然而，我不想列举更常见的错误或充实它们，而是想深入挖掘一点，讨论如何完全避免犯这些错误（或者非常快地修复它们）。这样做的诀窍是遵循一个特定的过程，据我所知，这个过程并不经常被记录下来。让我们从两个重要的观察开始，这些观察激励了它。
1)神经网络训练是一种漏水的抽象

据称，开始训练神经网络很容易。许多库和框架以显示30行奇迹片段而自豪，这些片段可以解决您的数据问题，给人一种（错误的）印象，即这些东西是即插即用的。常见的情况是：

```python
>>> your_data = # plug your awesome dataset here
>>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)
# conquer world here
```

这些库和示例激活了我们大脑中熟悉标准软件的部分--在这个地方，干净的API和抽象通常是可以实现的。请求要演示的库：

```python
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
```

太酷了！一个勇敢的开发人员从您身上承担了理解查询字符串、urls、GET/POST请求、HTTP连接等的重担，并在很大程度上隐藏了几行代码背后的复杂性。这就是我们熟悉和期待的。不幸的是，神经网络不是那样的。当你稍微偏离训练ImageNet分类器时，它们就不是“现成的”技术。我试图在我的帖子中表明这一点"是的，你应该理解背撑"通过挑起反向传播，称其为“漏水的抽象”，但不幸的是，情况要可怕得多。Backprop + SGD并不能神奇地使您的网络工作。批处理范数并不能神奇地使它收敛得更快。RNN不会神奇地让你“插入”文本。仅仅因为你可以把你的问题表述为RL并不意味着你应该这样做。如果你坚持使用这项技术，而不了解它的工作原理，你很可能会失败。这让我想到...
神经网络训练无声无息地失败

当您中断或错误配置代码时，您通常会得到某种异常。您插入了一个整数，其中有些东西需要一个字符串。该函数只需要3个参数。此导入失败。该密钥不存在。两个列表中的元素数量不相等。此外，通常可以为某些功能创建单元测试。

这只是训练神经网络的一个开始。一切都可以在句法上正确，但整个事情的安排并不正确，而且很难说。“可能的错误表面”是大的，逻辑的（与语法相反），并且对单元测试非常棘手。例如，在数据增强期间左右翻转图像时，可能忘记翻转标签。你的网络仍然可以（令人震惊的是）工作得很好，因为你的网络可以在内部学习检测翻转的图像，然后它左右翻转它的预测。或者，您的自回归模型意外地将它试图预测的东西作为输入，因为一个关闭的错误。或者，您试图剪切梯度，但却剪切了损失，导致在训练期间忽略离群值示例。或者，您从预先训练的检查点初始化了权重，但没有使用原始平均值。或者你只是搞砸了正则化强度、学习率、衰减率、模型大小等的设置。因此，你配置错误的神经网络只有在你幸运的时候才会抛出异常；大多数时候，它将训练，但默默地工作得更糟。

因此，（这真的很难过分强调）训练神经网络的“快速而狂暴”方法是行不通的只会导致痛苦。现在，痛苦是让神经网络很好地工作的一个非常自然的部分，但它可以通过彻底、防御、偏执和痴迷于基本上每一个可能的事情的可视化来缓解。在我的经验中，与深度学习成功最密切相关的品质是耐心和对细节的关注。
食谱

根据上述两个事实，我为自己开发了一个具体的过程，我在将神经网络应用于一个新问题时遵循这个过程，我将尝试描述这个过程。你会看到，它非常认真地对待上面的两个原则。特别是，它从简单到复杂，在每一步，我们都对将要发生的事情做出具体的假设，然后通过实验验证它们，或者进行调查，直到我们发现一些问题。我们努力防止的是同时引入许多“未经验证”的复杂性，这必然会引入错误/配置错误，这些错误/配置错误将需要很长时间才能找到（如果有的话）。如果编写神经网络代码就像训练一个一样，你会希望使用非常小的学习率和猜测，然后在每次迭代后评估完整的测试集。
1、与数据合二为一

训练神经网络的第一步是根本不接触任何神经网络代码，而是从彻底检查数据开始。这一步至关重要。我喜欢花大量的时间（以小时为单位）扫描成千上万的例子，了解它们的分布并寻找模式。幸运的是，你的大脑很擅长这个。有一次，我发现数据中包含重复的示例。还有一次，我发现了损坏的图像/标签。我寻找数据不平衡和偏见。我通常还将关注我自己对数据进行分类的过程，这暗示了我们最终将探索的体系结构类型。例如，非常本地的功能是否足够，还是我们需要全局上下文？有多少变化，它采取什么形式？哪些变化是杂散的，可以预处理出来？空间位置很重要吗？还是我们想把它平均起来？细节有多重要，我们能在多大程度上对图像进行降采样？标签有多吵？

此外，由于神经网络实际上是数据集的压缩/编译版本，您将能够查看网络（错误）预测，并了解它们可能来自哪里。如果你的网络给你的预测似乎与你在数据中看到的不一致，那就出了问题。

一旦你有了定性的感觉，写一些简单的代码来搜索/过滤/排序也是一个好主意（例如标签类型、注释大小、注释数量、等），并可视化它们沿任何轴的分布和离群值。异常值尤其是几乎总是发现数据质量或预处理中的一些错误。
2、建立端到端培训/评估框架+获取哑基线

现在我们了解了我们的数据，我们可以使用我们的超级花哨的多尺度ASPP FPN ResNet，并开始训练令人敬畏的模型吗？当然不，那是通往苦难的道路。我们的下一步是建立一个完整的训练+评估框架，并通过一系列实验获得对其正确性的信任。在这个阶段，最好选择一些你不可能以某种方式搞砸的简单模型--例如线性分类器，或者一个非常小的ConvNet。我们希望训练它，可视化损失，任何其他指标（例如准确性），模型预测，并在过程中执行一系列带有明确假设的消融实验。

此阶段的提示和技巧：

    固定随机种子。始终使用固定的随机种子，以保证当您运行两次代码时，您将得到相同的结果。这消除了一个变化因素，并将有助于保持你的理智。
    简化.确保禁用任何不必要的幻想。例如，在此阶段，一定要关闭任何数据增强。数据增强是一种正则化策略，我们稍后可能会加入，但目前这只是引入一些愚蠢错误的又一个机会。
    将有效数字添加到评估中.绘制测试损失时，在整个（大）测试集上运行评估。不要仅仅在批次上绘制测试损失，然后依靠Tensorboard中平滑它们。我们追求正确性，非常愿意放弃时间来保持理智。
    验证丢失@ init.验证您的损失是否从正确的损失值开始。例如，如果您正确初始化最后一层，您应该测量-log(1/n_classes)在初始化时的softmax上。L2回归、Huber损失等可以导出相同的默认值。
    初始化井.正确初始化最终图层权重。例如，如果您正在回归一些平均值为50的值，则将最终偏差初始化为50。如果您的数据集的正负比率为1:10，请在Logit上设置偏差，以便您的网络在初始化时预测概率为0.1。正确设置这些将加快收敛速度，并消除“曲棍球棒”损失曲线，在最初的几次迭代中，您的网络基本上只是学习偏差。
    人体基线.监控人类可解释和可检查的损失以外的指标（例如准确性）。尽可能评估您自己（人类）的准确性，并与之进行比较。或者，对测试数据进行两次注释，对于每个示例，将一个注释视为预测，将第二个注释视为基础真值。
    投入无关基线.训练一个与输入无关的基线（例如，最简单的是将所有输入设置为零）。这应该比实际插入数据而不将其归零时的性能更糟。是吗？即，您的模型是否学会从输入中提取任何信息？
    过拟合一批.仅对几个示例的单个批次进行过拟合（例如，只有两个示例）。为此，我们增加了模型的容量（例如添加层或过滤器），并验证我们可以达到最低的可实现损失（例如零）。我还喜欢在同一情节中可视化标签和预测，并确保一旦我们达到最小损失，它们最终完美对齐。如果他们不这样做，那就有一个错误，我们就不能继续下一阶段。
    验证减少训练损失.在这个阶段，您希望在数据集上不适合，因为您正在使用玩具模型。试着增加一点容量。你的训练损失下降了吗？
    就在网前可视化。可视化数据的正确位置是在您的y_hat = model(x)(或sess.run在TF中)。那就是-你想想象准确的说进入网络的内容，将数据和标签原始张量解码为可视化效果。这是唯一的“真理之源”。我数不清这拯救了我多少次，并揭示了数据预处理和增强方面的问题。
    可视化预测动力学.我喜欢在训练过程中可视化固定测试批次上的模型预测。这些预测如何移动的“动态”将给你难以置信的良好直觉，以了解训练的进展情况。很多时候，如果网络以某种方式摆动太多，就会感觉到网络“挣扎”来适应你的数据，从而揭示出不稳定。非常低或非常高的学习率也很容易在抖动量上明显。
    使用backprop绘制依赖关系图。您的深度学习代码通常包含复杂的、矢量化的和广播的操作。我遇到过几次比较常见的错误是，人们会弄错这个错误(例如，他们使用view而不是transpose/permute在某个地方)，并无意中在批处理维中混合信息。令人沮丧的事实是，您的网络通常仍然会训练正常，因为它将学会忽略其他示例中的数据。调试此问题（以及其他相关问题）的一种方法是将损失设置为微不足道的东西，如示例所有输出的总和我，将向后传递一直运行到输入，并确保仅在第i次输入。相同的策略可用于例如，确保时间t的自回归模型仅取决于1.t-1。更普遍的是，梯度为您提供了有关网络中哪些内容取决于内容的信息，这对于调试非常有用。
    概括特例这更像是一个通用的编码技巧，但我经常看到人们在咬掉的东西超过了他们所能咀嚼的东西时就会产生错误，从零开始编写一个相对通用的功能。我喜欢为我现在正在做的事情写一个非常具体的函数，让它起作用，然后在以后推广它，确保我得到相同的结果。这通常适用于矢量化代码，在这种代码中，我几乎总是先写出完全循环的版本，然后才将其转换为矢量化代码，一次循环一个。

3.过拟合

在这个阶段，我们应该对数据集有很好的了解，我们有完整的培训+评估管道工作。对于任何给定的模型，我们都可以（再现地）计算我们信任的度量。我们还装备了一个独立于输入的基线的性能，一些愚蠢的基线的性能（我们最好打败这些基线），我们对人类的性能有一个粗略的感觉（我们希望达到这个目标）。现在，在一个好的模型上迭代的阶段已经准备好了。

我喜欢采取的寻找好模型的方法有两个阶段：首先得到一个足够大的模型，可以过拟合（即关注训练损失），然后适当地将其正则化（放弃一些训练损失以改善验证损失）。我喜欢这两个阶段的原因是，如果我们无法在任何模型上达到低错误率，这可能会再次表明一些问题、错误或配置错误。

此阶段的一些提示和技巧：

    选择模型.要达到良好的训练损失，您需要为数据选择适当的体系结构。当谈到选择这个时，我的首要建议是：不要当英雄我见过很多人，他们渴望疯狂和创造性地把神经网络工具箱的乐高积木堆在各种对他们来说有意义的奇异建筑中。在项目的早期阶段，强烈抵制这种诱惑。我总是建议人们简单地找到最相关的纸张，并复制粘贴他们最简单的体系结构，以实现良好的性能。例如，如果您正在对图像进行分类，请不要成为英雄，只需复制粘贴ResNet-50进行第一次运行。你以后可以做一些更定制的事情，然后打败这个。
    亚当很安全.在设置基线的早期阶段，我喜欢使用Adam，学习率为3e-4根据我的经验，亚当对超参数更宽容，包括糟糕的学习率。对于ConvNets来说，调整良好的SGD几乎总是略微优于Adam，但最佳学习率区域要狭窄得多，而且特定于问题。（注意：如果您使用的是RNN和相关的序列模型，那么使用Adam更常见。在项目的初始阶段，再次，不要成为英雄，遵循最相关的论文所做的任何事情。）
    一次只复杂化一个。如果您有多个信号要插入分类器，我建议您一个接一个地插入它们，每次都确保您获得预期的性能提升。不要一开始就把厨房水槽扔到你的模特身上。还有其他的方法来建立复杂性--例如，你可以尝试先插入较小的图像，然后再将其放大，等等。
    不信任学习速率衰减默认值。如果您正在重新调整其他领域的代码用途，请务必非常小心学习速率衰减。您不仅希望对不同的问题使用不同的衰减计划，而且--更糟糕的是--在典型的实现中，计划将基于当前的时代号，这可能会根据数据集的大小而大不相同。例如，ImageNet将在第30时代衰减10。如果您不训练ImageNet，那么您几乎肯定不希望这样做。如果你不小心，你的代码可能会偷偷地过早地将你的学习率推至零，而不允许你的模型收敛。在我自己的工作中，我总是完全禁用学习速率衰减（我使用恒定的LR），并在最后一直调整它。

4.正则化

理想的情况是，我们现在处于一个地方，我们有一个大型模型，至少适合训练集。现在是时候对它进行正则化，并通过放弃一些训练精度来获得一些验证精度了。一些提示和技巧：

    获取更多数据首先，在任何实际环境中，正则化模型的最佳和首选方法是添加更多真实的训练数据。这是一个非常常见的错误，花大量的工程周期试图从一个小数据集中挤出果汁，而你可以收集更多的数据。就我所知，添加更多的数据几乎是单调地提高配置良好的神经网络性能的唯一保证方法。另一个是合奏（如果你买得起的话），但这在5个型号之后就会达到顶峰。
    数据增强.仅次于真实数据的最佳方法是半假数据--尝试更积极的数据增强。
    创造性增强.如果半假数据做不到，假数据也可能做一些事情。人们正在寻找扩展数据集的创造性方法；例如，域随机化、使用模拟、聪明混血儿例如将（潜在模拟的）数据插入场景，甚至GAN。
    预训练。如果可以的话，使用预训练的网络很少有什么坏处，即使你有足够的数据。
    坚持监督学习.不要对无监督的预训练过于兴奋。与2008年的博客文章告诉你的不同，据我所知，它没有一个版本的报告在现代计算机视觉中取得了强劲的结果（尽管NLP最近似乎在BERT和朋友们身上做得很好，很可能是由于文本的更深思熟虑的性质，以及更高的信噪比）。
    较小的输入维数.删除可能包含杂散信号的特征。如果数据集较小，任何添加的杂散输入都只是另一个过度拟合的机会。同样，如果低级细节并不重要，请尝试输入一个较小的图像。
    更小的型号尺寸。在许多情况下，您可以使用网络上的域知识约束来减小其大小。例如，在ImageNet的主干顶部使用全连接层过去是很流行的，但这些层后来被简单的平均池取代，消除了过程中的大量参数。
    减小批处理大小.由于批处理范数内部的规范化，较小的批处理大小在某种程度上对应于较强的正则化。这是因为批次经验平均值/std是完整平均值/std的更近似版本，因此比例和偏移“摆动”您的批次更多。
    掉落.添加dropout。对ConvNets使用dropout2d（空间dropout）。谨慎/小心使用，因为辍学似乎并不友好使用批处理标准化。
    重量衰减.增加重量衰减惩罚。
    早停.根据测量的验证损失停止训练，以便在模型即将过拟合时捕获模型。
    尝试更大的型号我最后提到这一点，也是在提前停止之后才提到的，但我过去发现过几次，大型型号最终当然会更适合，但它们的“提前停止”性能往往比小型型号要好得多。

最后，为了获得更多的信心，即您的网络是一个合理的分类器，我喜欢可视化网络的第一层权重，并确保您获得有意义的漂亮边缘。如果您的第一层过滤器看起来像噪音，那么可能会有一些东西关闭。同样，网络内的激活有时会显示奇怪的伪影，并暗示问题。
5.调音

现在，您应该“在循环中”与您的数据集探索一个广泛的模型空间，以实现低验证损失的体系结构。此步骤的一些提示和技巧：

    网格上随机搜索。对于同时调整多个超参数，使用网格搜索来确保覆盖所有设置听起来可能很诱人，但请记住，它是最好使用随机搜索代替直觉上，这是因为神经网络对某些参数的敏感度往往比其他参数高得多。在限制中，如果参数一个重要的是改变b没有效果，你宁愿试吃一个比在几个固定点多次更彻底。
    超参数优化.周围有大量花哨的贝叶斯超参数优化工具箱，我的一些朋友也报告了它们的成功。但我个人的经验是，探索模型和超参数空间的最先进的方法是使用实习生：)。开玩笑的。

6.榨出汁液

一旦您找到了最佳类型的体系结构和超参数，您仍然可以使用更多的技巧来挤出系统中的最后一部分：

    合奏.模型组合是一种几乎有保证的方法，可以在任何事情上获得2%的准确性。如果您在测试时负担不起计算费用，请考虑将您的集合提取到网络中，使用黑暗知识.
    让它训练.我经常看到，当验证损失似乎趋于平稳时，人们试图停止模型训练。根据我的经验，网络保持了很长时间的训练。有一次，我在寒假期间不小心离开了一个模特训练，当我在一月份回来的时候，它是SOTA（“最先进的”）。

结论

一旦你成功了，你就会拥有成功的所有要素：你对技术、数据集和问题有了深刻的了解，你已经建立了整个培训/评估基础架构，并对其准确性有了高度的信心，你已经探索了越来越复杂的模型，以您预测的每一步方式获得性能改进。你现在已经准备好阅读大量的论文，尝试大量的实验，并得到你的SOTA结果。祝你好运！

